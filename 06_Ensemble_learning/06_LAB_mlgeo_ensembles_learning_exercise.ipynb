{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ae5814",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Machine Learning in Geosciences \n",
    "Department of Applied Geoinformatics and Carthography, Charles University\n",
    "\n",
    "Lukas Brodsky lukas.brodsky@natur.cuni.cz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f756b",
   "metadata": {},
   "source": [
    "## Exercise: Building and Evaluating Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b1dff",
   "metadata": {},
   "source": [
    "This notebook is dedicated to ensemble learning exercizes. \n",
    "\n",
    "**Objective**:\n",
    "Understand and implement different ensemble learning techniques â€” Bagging, Boosting, and Stacking and compare their performance.\n",
    "\n",
    "Tasks: \n",
    "1. Implement Bagging using `BaggingClassifier()` and compare the result with a weak classifier, e.g. `DecisionTreeClassifier()` using high variance (noisy moons) dataset. \n",
    "\n",
    "2. Implement Boosting using `GradientBoostingClassifier()` and compare result with a weak classifier `DecisionTreeClassifier()` on  a complex decision boundary (circles) dataset. \n",
    "\n",
    "3. Implement Stacking using `StackingClassifier()` based on `SVC()`, `DecisionTreeClassifier()` and `KNeighborsClassifier()` and `LogisticRegression()`. Compare the stacking result with single weak classifiers, e.g. `DecisionTreeClassifier()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ensmbles \n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "# base estimators \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f75b0f",
   "metadata": {},
   "source": [
    "### Dataset 1 - bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c27126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 for BAGGING: High Variance (Noisy Moons)\n",
    "X_bagging, y_bagging = make_moons(n_samples=1000, noise=0.4, random_state=42)\n",
    "X_train_bag, X_test_bag, y_train_bag, y_test_bag = train_test_split(X_bagging, y_bagging, \n",
    "                                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_bagging[:, 0], X_bagging[:, 1], c=y_bagging, cmap=\"viridis\", alpha=0.5)\n",
    "plt.title(\"Dataset: High Variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d3da2",
   "metadata": {},
   "source": [
    "### Bagging classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd414d1b",
   "metadata": {},
   "source": [
    "**A Bagging classifier** is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "`\n",
    "class sklearn.ensemble.BaggingClassifier(\n",
    "    estimator=None, \n",
    "    n_estimators=10, \n",
    "    max_samples=1.0,\n",
    "    max_features=1.0, \n",
    "    bootstrap=True,\n",
    "    n_jobs=None\n",
    "    )\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5343e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters tunes number of estimators, sample size, and bootstrap settings.\n",
    "bagging_params = {\n",
    "    \"n_estimators\": [50, 100, 150, 200],\n",
    "    \"max_samples\": [0.5, 0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GridSearchCV() on BaggingClassifier() with DecisionTreeClassifier()\n",
    "# cv=10, return_train_score=True \n",
    "bagging_grid = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01b061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search and fit \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a494e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best paramters are: estimator.best_params_\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best one is: estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d305a5c2",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95530513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training score: estimator.best_score_\n",
    "BC_train_acc = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bd24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for testing \n",
    "bagging_pred = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy: accuracy_score(y_test_bag, bagging_pred)\n",
    "BC_test_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the accuracies\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cfa75",
   "metadata": {},
   "source": [
    "#### Plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785bbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Decision Boundary\n",
    "def plot_ensmble_boundary(model, X, y, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    for model in model.estimators_:\n",
    "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        cs = plt.contourf(xx, yy, Z, alpha=0.1)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", marker=\"o\", alpha=0.3)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ensmble_boundary(bagging_best, X_test_bag, y_test_bag, \"Bagging Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86aa9d1",
   "metadata": {},
   "source": [
    "### Single model Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc22f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT DecisionTreeClassifier()  \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e372752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d156b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy: accuracy_score()\n",
    "DT_train_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d34dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy: accuracy_score()\n",
    "DT_test_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Boundary Plot\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", marker=\"o\", alpha=0.3)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.5)\n",
    "\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(dt, X_test_bag, y_test_bag, \"Single DT Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50386f2",
   "metadata": {},
   "source": [
    "#### Models comparison (accuarcy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Models Comparision =====\")\n",
    "print(f'Bagging training error: {round(BC_train_acc, 2)}')\n",
    "print(f'Bagging testing  error: {round(BC_test_acc , 2)}')\n",
    "print('---')\n",
    "print(f'Decition Tree training: {round(DT_train_acc, 2)}')\n",
    "print(f'Decition Tree training: {round(DT_test_acc, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889624a8",
   "metadata": {},
   "source": [
    "### Does averaging weak learners reduces overfitting on noisy data? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb8628",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22705ab",
   "metadata": {},
   "source": [
    "### Boosting classifier \n",
    "\n",
    "**Gradient Boosting** for classification builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. \n",
    "\n",
    "`\n",
    "class sklearn.ensemble.GradientBoostingClassifier(\n",
    "    n_estimators=100, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=3\n",
    "    )\n",
    "`\n",
    "\n",
    "where learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators. Values must be in the range [0.0, inf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb4664c",
   "metadata": {},
   "source": [
    "### Dataset 2 - boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2 for BOOSTING: Complex Decision Boundary (Circles)\n",
    "X_boosting, y_boosting = make_circles(n_samples=1000, noise=0.2, factor=0.5, random_state=42)\n",
    "X_train_boost, X_test_boost, y_train_boost, y_test_boost = train_test_split(X_boosting, y_boosting, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ea4d9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_boosting[:, 0], X_boosting[:, 1], c=y_boosting, cmap=\"viridis\", alpha=0.5)\n",
    "plt.title(\"Dataset: Complex Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76551da",
   "metadata": {},
   "source": [
    "### Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting model\n",
    "# boosting_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd090f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting: optimizes tree depth, learning rate, and number of estimators\n",
    "boosting_params = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [2, 3, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run GridSearchCV() on GradientBoostingClassifier \n",
    "# cv=5\n",
    "boosting_grid = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d9e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit through grid search CV \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best one: boosting_estimator.best_estimator_\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16179e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training score: boosting_estimator.best_score_\n",
    "Boost_train_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0c8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model \n",
    "boosting_pred = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ddd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing acccuracy accuracy_score()\n",
    "Boost_test_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d107ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c306c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(boosting_best, X_boosting, y_boosting, \"Boosting single Decision Boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Plot Decision Boundaries at Different Stages\n",
    "def plot_decision_boundaries_ensemble(model, X, y, stages=[1, 5, 50, 100], \n",
    "                                      title=\"Gradient Boosting Evolution\"):\n",
    "    h = 0.02  # Step size in meshgrid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(stages), figsize=(15, 4))\n",
    "    for ax, stage in zip(axes, stages):\n",
    "        # Partial predictions using first `stage` trees\n",
    "        stage_model = GradientBoostingClassifier(n_estimators=stage, learning_rate=0.1, random_state=42)\n",
    "        stage_model.fit(X, y)\n",
    "        Z = stage_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # Plot decision boundary\n",
    "        ax.contourf(xx, yy, Z, alpha=0.5)\n",
    "        ax.set_title(f\"{stage} Model(s)\")\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", marker=\"o\", alpha=0.1)\n",
    "    plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries_ensemble(boosting_best, X_boosting, y_boosting, stages=[1, 5, 20, 100], \n",
    "                                  title=\"Gradient Boosting Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0daec",
   "metadata": {},
   "source": [
    "### Weaker classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier()\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f02bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb304508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy\n",
    "DT_train_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438e40e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(dt, X_train_boost, y_train_boost, \"Single DT Decision Boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bf0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Models Comparision =====\")\n",
    "print(f'Boosting training error: {round(Boost_train_acc, 2)}')\n",
    "print(f'Boosting testing  error: {round(Boost_test_acc, 2)}')\n",
    "print('---')\n",
    "print(f'Decition Tree training: {round(DT_train_acc, 2)}')\n",
    "print(f'Decition Tree training: {round(DT_test_acc, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b6af8",
   "metadata": {},
   "source": [
    "### Does boosting improves weak model errors in overlapping, complex patterns? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222045c",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c222b5c",
   "metadata": {},
   "source": [
    "### Stacking \n",
    "\n",
    "**Stack of estimators with a final classifier** consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n",
    "\n",
    "`\n",
    "class sklearn.ensemble.StackingClassifier(\n",
    "    estimators,\n",
    "    final_estimator\n",
    "    )\n",
    "`\n",
    "\n",
    "wehere `estimators` are base estimators, \n",
    "and `final_estimator` is a classifier which will be used to combine the base estimators. The default classifier is a LogisticRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f9fd7e",
   "metadata": {},
   "source": [
    "### Dataset 3 - stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 3 for STACKING: Diverse Model Mistakes (Classification)\n",
    "X_stacking, y_stacking = make_classification(n_samples=1000, n_features=10, n_informative=3, class_sep=0.5, \n",
    "                                             n_redundant=2, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "X_train_stack, X_test_stack, y_train_stack, y_test_stack = train_test_split(X_stacking, y_stacking, \n",
    "                                                                            test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SVM, DT, and KNN \n",
    "stacking_estimators = [\n",
    "    ('svm', SVC(probability=True, kernel='rbf', random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f364b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking: fine-tunes Logistic Regression used as the meta-classifier. \n",
    "stacking_params = {\n",
    "    \"final_estimator__C\": [0.1, 1, 10]  # regularization strength for Logistic Regression\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run GridSearchCV() on StackingClassifier() using e.g. cv=10 \n",
    "stacking_grid = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search and fit \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2790a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best one is: .best_estimator_\n",
    "stacking_best = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training score: .best_score_\n",
    "Stack_train_acc = spass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64de57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing: \n",
    "stacking_pred = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3be210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy\n",
    "Stack_test_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763189e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0cfc03",
   "metadata": {},
   "source": [
    "### Weaker classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce0e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM: SVC(probability=True, kernel='rbf', random_state=42) \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ec9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f132f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy\n",
    "SVM_train_acc = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc4f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy \n",
    "SVM_test_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT: DecisionTreeClassifier()  \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b47129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy \n",
    "DT_train_acc = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy \n",
    "DT_test_acc = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a76416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN: KNeighborsClassifier(n_neighbors=10)\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training accuracy \n",
    "KNN_train_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing accuracy \n",
    "KNN_test_acc = pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Models Comparision =====\")\n",
    "print(f'Stacking training error: {round(Stack_train_acc, 2)}')\n",
    "print(f'Stacking testing  error: {round(Stack_test_acc, 2)}')\n",
    "print('===')\n",
    "print(f'SVM model training: {round(SVM_train_acc, 2)}')\n",
    "print(f'SVM model testing: {round(SVM_test_acc, 2)}')\n",
    "print('---')\n",
    "print(f'Decition Tree training: {round(DT_train_acc, 2)}')\n",
    "print(f'Decition Tree testing: {round(DT_test_acc, 2)}')\n",
    "print('---')\n",
    "print(f'KNN model training: {round(KNN_train_acc, 2)}')\n",
    "print(f'KNN model testing: {round(KNN_test_acc, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d51db",
   "metadata": {},
   "source": [
    "### Does boosting improves weak model errors in overlapping, complex patterns? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8cbcd",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf9255e",
   "metadata": {},
   "source": [
    "### Final thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6cecf",
   "metadata": {},
   "source": [
    "**1. Bagging (Random Forest)** \n",
    "   \n",
    "Questions: \n",
    "\n",
    "    Is the model robust on noisy data? \n",
    "    Is it true that the high-variance dataset benefits from model averaging? \n",
    "    Is it true that Bagging leads to less overfitting compared to a single decision tree.\n",
    "    \n",
    "\n",
    "**2. Boosting** \n",
    "\n",
    "Questions: \n",
    "\n",
    "    Is it true that it corrects mistakes iteratively, capturing non-linear interactions better? \n",
    "    Is it true that it would outperform traditional models like Decision Trees.\n",
    "    \n",
    "\n",
    "**3. Stacking** \n",
    "\n",
    "Questions: \n",
    "\n",
    "    Does it works better when different models make different errors? \n",
    "    Is it true that stacking leverages these complementary strengths for improved accuracy? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
