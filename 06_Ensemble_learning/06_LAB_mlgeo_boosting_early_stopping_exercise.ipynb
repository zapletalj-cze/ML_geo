{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9dfb6f7",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Machine Learning in Geosciences ] \n",
    "Department of Applied Geoinformatics and Carthography, Charles University\n",
    "\n",
    "Lukas Brodsky lukas.brodsky@natur.cuni.cz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09c6b0",
   "metadata": {},
   "source": [
    "## Exercise: Boosting Early Stopping technique\n",
    "\n",
    "This notebook is dedicated to early stopping in boosting model . \n",
    "\n",
    "**Objective**:\n",
    "Understand and implement different ensemble learning techniques—Bagging, Boosting, and Stacking—on a real-world dataset and compare their performance.\n",
    "\n",
    "Tasks: \n",
    "1. Implement Gradient Boosting algorithm based on sklearn `GradientBoostingRegressor` class. \n",
    "2. Run boosting model with up to 200 estimators and measure testing error. \n",
    "3. Implement **Early stoping** procedure and plot the model performance with the indicator of the stopped boosting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d947e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bab112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce1ba8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=49)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31abefac",
   "metadata": {},
   "source": [
    "### Gradient Boosting Model - error evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f4d04",
   "metadata": {},
   "source": [
    "**To implement:** \n",
    "\n",
    "`\n",
    "Algorithm GradientBoosting:\n",
    "Initialize GradientBoostingRegressor with:\n",
    "        max_depth = 2\n",
    "        warm_start = True\n",
    "        random_state = 42\n",
    "\n",
    "    Initialize train_err as an empty list\n",
    "    Initialize test_err as an empty list\n",
    "\n",
    "    For n_estimators from 1 to 199 do:\n",
    "        Set gbrt.n_estimators to n_estimators\n",
    "        Train gbrt using X_train and y_train\n",
    "\n",
    "        // Compute training error\n",
    "        Predict y_train_pred using gbrt on X_train\n",
    "        Compute train_error as mean squared error between y_train and y_train_pred\n",
    "        Append train_error to train_err list\n",
    "\n",
    "        // Compute test error\n",
    "        Predict y_pred using gbrt on X_test\n",
    "        Compute test_error as mean squared error between y_test and y_pred\n",
    "        Append test_error to test_err list\n",
    "End Algorithm\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure validation error of Boosting model with up to 200 estimators \n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of the validation errors \n",
    "plt.plot(list(range(1, 200)), train_err, 'g-', label='Training error')\n",
    "plt.plot(list(range(1, 200)), test_err, 'b-', label='Testing error')\n",
    "plt.ylim(0.0, 0.008)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.title('Training and testing error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410522ec",
   "metadata": {},
   "source": [
    "## When to stop the model learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9a27ff",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a963fa2",
   "metadata": {},
   "source": [
    "### Implement early stopping procedure with parameter testing error going up = 5 to find the best!\n",
    "\n",
    "Stop after model testing error increses for **five times** in the iterative learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132c842",
   "metadata": {},
   "source": [
    "**Early stopping:**\n",
    "\n",
    "`\n",
    "Algorithm EarlyStopping:\n",
    "    If val_error < min_val_error Then:\n",
    "        Set min_val_error to val_error\n",
    "        Set error_going_up to 0\n",
    "    Else:\n",
    "        Increment error_going_up by 1\n",
    "        If error_going_up equals 5 Then:\n",
    "            Break the loop\n",
    "End Algorithm\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd2db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the early stopping algorithm \n",
    "# mean_squared_error(y_test, y_pred)\n",
    "# min_val_error = float(\"inf\") \n",
    "# add error_going_up = 0 \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33b2f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gbrt.n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840d81e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the evolution of the validation errors \n",
    "plt.plot(list(range(1, 200)), train_err, 'g-', label='Training error')\n",
    "plt.plot(list(range(1, 200)), test_err, 'b-', label='Testing error')\n",
    "plt.axvline(gbrt.n_estimators, color = 'red', label = 'STOP')\n",
    "plt.ylim(0.0, 0.008)\n",
    "plt.xlabel('Number of trees')\n",
    "plt.title('Training and testing error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ac087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model plot data \n",
    "X_sim = np.linspace(-0.5, 0.5, 100)\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ad03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction plot \n",
    "pass \n",
    "# plot data \n",
    "# plot model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
