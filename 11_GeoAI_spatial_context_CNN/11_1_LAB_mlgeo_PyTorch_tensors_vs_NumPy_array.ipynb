{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4408fd59",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Machine Learning in Geosciences ] \n",
    "Department of Applied Geoinformatics and Carthography, Charles University\n",
    "\n",
    "Lukas Brodsky lukas.brodsky@natur.cuni.cz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e259d",
   "metadata": {},
   "source": [
    "### PyTorch installation\n",
    "\n",
    "`pip install torch`\n",
    "`pip install torchvision`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aff0d45",
   "metadata": {},
   "source": [
    "### PyTorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d80faf1",
   "metadata": {},
   "source": [
    "Tensors are the building blocks for representing data in PyThorch. It is the fundamental data structure. The term `tensor` comes bundled with the notion of spaces. In this context of deep learning, tensors refer to the generalization of vectors and matrices to an arbitrary number of dimensions. \n",
    "\n",
    "The torch package contains not only the data structures for **multi-dimensional arrays** but also defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a900d",
   "metadata": {},
   "source": [
    "### PyTorch tensor vs. NumPy array\n",
    "\n",
    "If you're familiar with NumPy arrays, transitioning to PyTorch tensors is smooth—many operations are conceptually and syntactically similar. However, PyTorch brings additional power, especially for deep learning and GPU-accelerated computations.\n",
    "\n",
    "What is the difference between numpy array and pytorch tensor?\n",
    "\n",
    "1. The numpy arrays are the core functionality of the numpy package designed to support faster mathematical operations. Pytorch tensors are similar to numpy arrays, but can also be operated on CUDA-capable Nvidia GPU.\n",
    "   \n",
    "   \n",
    "2. Numpy arrays are mainly used in typical machine learning algorithms. Pytorch tensors are mainly used in deep learning which requires heavy matrix computation.\n",
    "\n",
    "3. Unlike numpy arrays, while creating pytorch tensor, it also accepts two other arguments called the device_type (whether the computation happens on CPU or GPU) and the requires_grad (which is used to compute the derivatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db7fd29",
   "metadata": {},
   "source": [
    "#### PyTorch API\n",
    "\n",
    "The PyTorch API establish a few directions on where to find things in the documentation (https://pytorch.org/docs/stable/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f42689",
   "metadata": {},
   "source": [
    "## What’s the Same? \n",
    "### PyTorch vs. NumPy\n",
    "**Syntax and behavior**: PyTorch tensors and NumPy arrays share similar syntax for indexing, slicing, reshaping, broadcasting, element-wise operations, and reductions (e.g., sum, mean, max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d19fbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20344c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = np.array([[1, 2], [3, 4]])\n",
    "a_torch = torch.tensor([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c05aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3]\n",
      "tensor([1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Both support slicing\n",
    "print(a_np[:, 0])        # [1 3]\n",
    "print(a_torch[:, 0])     # tensor([1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7150e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeros and ones\n",
    "np_zeros = np.zeros((2, 3))\n",
    "torch_zeros = torch.zeros((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3d3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ones = np.ones((2, 3))\n",
    "torch_ones = torch.ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c5f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity matrix\n",
    "np_eye = np.eye(3)\n",
    "torch_eye = torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d89d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Shape and Reshaping\n",
    "# Shape attribute\n",
    "print(a_np.shape)       # (2, 2)\n",
    "print(a_torch.shape)   # torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5724d92",
   "metadata": {},
   "source": [
    "### Data types and shapes: \n",
    "both support multiple data types (float32, int64, etc.) and n-dimensional arrays (tensors) with shape attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c025e2",
   "metadata": {},
   "source": [
    "### Interoperability: \n",
    "PyTorch and NumPy arrays can be converted back and forth easily (when on CPU):\n",
    "\n",
    "**torch.from_numpy()**\n",
    "\n",
    "**a_torch.numpy()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53ad387",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np = np.array([1, 2, 3])\n",
    "a_torch = torch.from_numpy(a_np)  # Shares memory!\n",
    "a_np_back = a_torch.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902c0d3",
   "metadata": {},
   "source": [
    "## What’s Similar?\n",
    "\n",
    "**Broadcasting rules**: PyTorch follows NumPy-style broadcasting rules for arithmetic between arrays of different shapes.\n",
    "\n",
    "**Linear algebra operations**: Both provide high-level APIs for matrix multiplication (@ or .matmul()), dot product, transposition, etc.\n",
    "\n",
    "**Random number generation**: Both libraries offer similar random sampling utilities (though APIs differ slightly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca74700",
   "metadata": {},
   "source": [
    "### Broadcasting \n",
    "is a technique that allows arrays (or tensors) of different shapes to be used in arithmetic operations without explicit replication. It's like virtually expanding the smaller array to match the shape of the larger one.\n",
    "\n",
    "**When operating on two tensors:**\n",
    "\n",
    "**1. Right-align the shapes.**\n",
    "\n",
    "`\n",
    "A: (3, 1, 5)\n",
    "B: (   1, 5)   ← B is treated as (1, 1, 5)\n",
    "\n",
    "`\n",
    "\n",
    "**2. Dimensions must match or be 1 (which means that dimension can be broadcast).**\n",
    "`\n",
    "\n",
    "A.shape = (4, 1, 6)\n",
    "\n",
    "B.shape = (1, 5, 6)\n",
    "\n",
    "dim 1: 4 vs 1 → ok, broadcast B\n",
    "\n",
    "dim 2: 1 vs 5 → ok, broadcast A\n",
    "\n",
    "dim 3: 6 vs 6 → ok, match\n",
    "\n",
    "C = A + B\n",
    "\n",
    "print(C.shape)  # torch.Size([4, 5, 6])\n",
    "\n",
    "`\n",
    "\n",
    "`\n",
    "A.shape = (2, 3)\n",
    "\n",
    "B.shape = (4, 3)\n",
    "\n",
    "dim 1: 2 vs 4 → NOK, Not equal, neither is 1 → Error\n",
    "\n",
    "dim 2: 3 vs 3 → Ok\n",
    "\n",
    "`\n",
    "\n",
    "**3. If a dimension doesn't exist (because one tensor has fewer dimensions), it’s treated as 1.** \n",
    "`\n",
    "A.shape = (3, 4, 5)\n",
    "\n",
    "B.shape =      (4, 5)\n",
    "\n",
    "3 vs 1 → ok, broadcast\n",
    "\n",
    "4 vs 4 → ok\n",
    "\n",
    "5 vs 5 → ok\n",
    "\n",
    "A = torch.randn(3, 4, 5)\n",
    "\n",
    "B = torch.randn(4, 5)\n",
    "\n",
    "C = A + B  # Broadcasts B to (3, 4, 5)\n",
    "\n",
    "print(C.shape)  # torch.Size([3, 4, 5])\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900f96c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11., 12.],\n",
      "        [13., 14.]])\n"
     ]
    }
   ],
   "source": [
    "# Adding a scalar\n",
    "a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = 10.0\n",
    "\n",
    "print(a + b)\n",
    "# tensor([[11., 12.],\n",
    "#         [13., 14.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03fb2032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11., 22.],\n",
      "        [13., 24.]])\n"
     ]
    }
   ],
   "source": [
    "#  2D + 1D\n",
    "a = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = torch.tensor([10.0, 20.0])  # Shape: (2,)\n",
    "\n",
    "print(a + b)\n",
    "# tensor([[11., 22.],\n",
    "#         [13., 24.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a820e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incompatible shapes\n",
    "a = torch.ones(3, 4)\n",
    "b = torch.ones(5, 1)\n",
    "\n",
    "# This will raise an error:\n",
    "# print(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20208885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch (like NumPy), broadcasting only works when shapes can be aligned \n",
    "# and broadcasted according to the rules!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c7f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 3D tensor broadcasting\n",
    "a = torch.rand(1, 3, 1)  # Shape: (1, 3, 1)\n",
    "b = torch.rand(2, 1, 4)  # Shape: (2, 1, 4)\n",
    "\n",
    "# Broadcast result will be shape (2, 3, 4)\n",
    "c = a + b\n",
    "print(c.shape)  # torch.Size([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7779cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n",
      "tensor([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Math Operations\n",
    "a_np = np.array([1, 2, 3])\n",
    "a_torch = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Element-wise addition\n",
    "print(a_np + 1)       # [2 3 4]\n",
    "print(a_torch + 1)    # tensor([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "113fcd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6]\n",
      "tensor([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "print(a_np * 2)       # [2 4 6]\n",
    "print(a_torch * 2)    # tensor([2, 4, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1afba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "# Dot product\n",
    "print(np.dot(a_np, a_np))       # 14\n",
    "print(torch.dot(a_torch, a_torch))  # tensor(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba3b538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 10]\n",
      " [15 22]]\n",
      "tensor([[ 7, 10],\n",
      "        [15, 22]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "mat_np = np.array([[1, 2], [3, 4]])\n",
    "mat_torch = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "print(mat_np @ mat_np)             # [[ 7 10] [15 22]]\n",
    "print(torch.matmul(mat_torch, mat_torch))  # same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "056784ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random values\n",
    "np_random = np.random.rand(2, 3)\n",
    "torch_random = torch.rand(2, 3)  # Uniform [0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ebd97",
   "metadata": {},
   "source": [
    "## What’s Different?\n",
    "\n",
    "**PyTorch:**\n",
    "\n",
    "* GPU support: `.to(\"cuda\")`\n",
    "* Autograd: `requires_grad=True`\n",
    "* Deep learning native: Integrated with models, optimizers, loss functions\n",
    "* In-place ops: Many functions end with _ (e.g., add_()) for memory efficiency\n",
    "* Memory sharing: Tensor and NumPy can share memory (CPU only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of GPU usage:\n",
    "# a = torch.tensor([1.0, 2.0], device='cuda')  # Runs on GPU\n",
    "a = torch.tensor([1.0, 2.0], device=\"\")  # Runs on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bf77299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.])\n"
     ]
    }
   ],
   "source": [
    "# Example of enabling gradient tracking:\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**2 + 3*x\n",
    "y.backward()\n",
    "print(x.grad)  # Computes dy/dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IF_HydroSim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
