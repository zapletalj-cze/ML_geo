{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___\n",
    "\n",
    "# [ Machine Learning in Geosciences ]\n",
    "\n",
    "**Department of Applied Geoinformatics and Carthography, Charles University** \n",
    "\n",
    "*Lukas Brodsky lukas.brodsky@natur.cuni.cz*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Regularized Polynomial Regression \n",
    "\n",
    "\n",
    "Input data: X .. change of water level of a dam, y .. water flowing out of the dam. \n",
    "\n",
    "Goal: model the regression relationshop. Generalize the polynomial model by explicit regularization of the Loss. \n",
    "\n",
    "Tasks: \n",
    "\n",
    "1. Develop Loss function with $l2$ regularization term for polynomial regression;  \n",
    "\n",
    "2. Run linear regression, plot the resulting model and learning curve (training and testing error agains the number of samples); \n",
    "\n",
    "3. Run polynomial regression (degree = 5), plot the resulting model, and the learning curve; \n",
    "\n",
    "4. Regularized the model with lambda = 2, plot the resulting model, and the learning curve; \n",
    "\n",
    "5. Regularized the model with lambda = 100, plot the resulting model, and the learning curve; \n",
    "\n",
    "6. Discuss the effect of regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# optimization procedure\n",
    "from scipy.optimize import minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = loadmat('./ex7data.mat')\n",
    "X_train = np.c_[np.ones_like(data['X']), data['X']]\n",
    "y_train = data['y']\n",
    "\n",
    "X_test = np.c_[np.ones_like(data['Xval']), data['Xval']]\n",
    "y_test = data['yval']\n",
    "\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot input data \n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train[:, 1], y_train, s = 50, c = 'red', marker = 'o', linewidths = 1, label = 'Data')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Change in water level (x)')\n",
    "plt.ylabel('Water flowing out of the dam (y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "The data are definitely not linear, but let's define the linear regression loss function with a second term, the regularization term that controls overfitting and underfitting. A higher $\\lambda$ value means smaller parameter ($\\theta$) values (danger of underfitting) and vice versa (danger of overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression Loss Function\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{2m} \\Big(\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2\\Big) + \\frac{\\lambda}{2m}\\Big(\\sum_{j=1}^n \\theta_j^2\\Big)$$\n",
    "\n",
    "where $h_\\theta(x)$ is the linear model defined by:\n",
    "\n",
    "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... +  \\theta_n x_n $$\n",
    "\n",
    "where $m$ is the number of training examples, $n$ is the number of parameters, and $\\theta_0$ is the bias term which goes unregularized.\n",
    "\n",
    "In order to minimize this loss function, we also need to calculate the partial derivatives of $L$ with respect to each $\\theta_{j}$.\n",
    "\n",
    "### Linear Regression Partial Derivatives\n",
    "$ \\frac{\\partial L(\\theta)}{\\partial\\theta_{j}} = \\frac{1}{m}\\sum_{i=1}^{m} \\Big( h_\\theta (x^{(i)})-y^{(i)}\\Big)x^{(i)}_{j} $      for $ j = 0 $\n",
    "\n",
    "$ \\frac{\\partial L(\\theta)}{\\partial\\theta_{j}} = \\Big(\\frac{1}{m}\\sum_{i=1}^{m}  ( h_\\theta (x^{(i)})-y^{(i)})x^{(i)}_{j}\\Big) + \\frac{\\lambda}{m}\\theta_j$ for $ j \\geq1 $\n",
    "\n",
    "These definitions are different because we do not regularize $\\theta_0$, the bias term.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Develop Loss function with $l2$ regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function taking into account the regularization term initialized to 0 \n",
    "# L_total = Lmse + L_reg \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  initialize theta as a vector of ones with a dimensionality equal to the number of columns in our matrix $X$. Then we can just pass the function these variables and see what it outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the losss() function with some data \n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize\n",
    "\n",
    "Now, we optimize our parameter values using the scipy function `minimize()`, taking as parameters: \n",
    "\n",
    "`fun = loss` .. loss function, as defined \n",
    "\n",
    "`x0 = theta` .. initial values of model parmeters theta \n",
    "\n",
    "`args = (X, y, reg)` inputs \n",
    "\n",
    "`jac = True` .. jac : bool or callable, optional Jacobian (gradient) of objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization procedure using scipy minimize()\n",
    "# Return optimal theta \n",
    "\n",
    "def optimalTheta(theta, X, y, reg = 0):\n",
    "    res = minimize(fun = loss, x0 = theta, args = (X, y, reg),  jac=True)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: REMOVE IT! \n",
    "def test_loss(x, y, w, reg=0):\n",
    "    m = len(x)\n",
    "    total_error = 0.0 \n",
    "    for i in range(m):\n",
    "        total_error += (y[i] - (w*x[i]))**2\n",
    "    J_total = total_error / m\n",
    "    \n",
    "    return J_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: REMOVE IT! \n",
    "def test_optimalTheta(theta, X, y, reg = 0):\n",
    "    res = minimize(fun = test_loss, x0 = theta, args = (X, y, reg),  jac = True)\n",
    "    \n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: REMOVE IT! \n",
    "initial_theta = 0\n",
    "theta_optim = test_optimalTheta(initial_theta, X_train[:,1], y_train)\n",
    "print(theta_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Run linear regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model (linear, generic)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the optimal paramters \n",
    "# initialize theta \n",
    "pass\n",
    "# optimize \n",
    "pass\n",
    "# print the result as a test \n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot e.g. (-50, 50, 100)\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result (data points and the model)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curve\n",
    "\n",
    "Let's compare our modelâ€™s training error with the validation error. These error values are the numbers output by our loss function relative to both datasets. One way to diagnose bias and variance errors is to plot the error relative to the training set and the validation data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get training and testing error against the increasing number of training data \n",
    "# from 1 to range of y_train.size \n",
    "# store the training and testing error for later plot \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the changing model trainin and testing error against the increasing number of training examples \n",
    "# (x: Number of training examples, y: Loss)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our error is high when the number of training examples increases, which is the exact opposite of what we should expect to see. This is definitely a high bias problem. Linear regression is just too simple to fit this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Run polynomial regression\n",
    "\n",
    "Addressing a simple linear model means adding more features which are no longer linear, but are instead the features being raised to higher powers. Explicitly, we will redefine $h$ as\n",
    "\n",
    "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 * (waterLevel) + \\theta_2 * (waterLevel)^2 + ... +  \\theta_p * (waterLevel)^p $$\n",
    "$$ = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2^2 + ... +  \\theta_n x_n^p $$\n",
    "\n",
    "... we are  squaring, cubing, etc the columns of our $X$ matrix and then multiplying these new features by new parameters $\\theta$ and adding all that together. This is called **feature mapping** -- we are mapping the old features to a higher dimension. Let's code it up! We will also code up **feature normalization**, which is defined as\n",
    "\n",
    "$$ X_{norm} = \\frac{(X - \\mu)}{\\sigma} $$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation. We do feature normalization because after feature mapping, the values need to be rescaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transformation function to given degree of polynomial \n",
    "# and feature normalization \n",
    "\n",
    "def polyFeatures(X, p):\n",
    "    \"\"\"Function to transform input features X into \n",
    "       polynomial of defiend degree and  normalize the features! \n",
    "       Returns both, original X and the transformed normalizaed X_norm\n",
    "    \"\"\"\n",
    "    for i in np.arange(p):\n",
    "        dim = i + 2\n",
    "        X = np.insert(X, X.shape[1], np.power(X[:,1], dim), axis = 1)\n",
    "    \n",
    "    X_norm = X\n",
    "    # Column wise\n",
    "    means = np.mean(X_norm, axis=0)\n",
    "    X_norm[:, 1:] = X_norm[:, 1:] - means[1:]\n",
    "    stds = np.std(X_norm, axis = 0)\n",
    "    X_norm[:, 1:] = X_norm[:, 1:] / stds[1:]\n",
    "    \n",
    "    return X, X_norm    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to fit theta to our new higher dimensional $X$ matrix and plot the fit. \n",
    "\n",
    "### Fit polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design decision is to use polynomial model with degree 5 \n",
    "degree = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare polynomial X (transform) \n",
    "pass \n",
    "\n",
    "# Initialize theta \n",
    "pass \n",
    "\n",
    "# Optimize \n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot data and the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot (linear data X from -50 to 50)\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot figures (data and resulting model)\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is definitely low training error, however, this fit is far too complex. The model is overfited (high variance), so we now have the opposite problem. Let's plot the new learning curve.\n",
    "\n",
    "#### Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare polynomial X_train and X_test\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use (re-define) function to get training and testing error against the increasing number of training data \n",
    "# from 1 to range of y.size \n",
    "# update the initial theta \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result (x: Number of training examples, y: Loss)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training error is basically zero, but it would also be nice if the validation error was also low. The model does not generalize well to data that were not used to optimize its parameter values. \n",
    "\n",
    "### Task 4. Regularize the model \n",
    "\n",
    "and adjust regularizing parameter $\\lambda$\n",
    "\n",
    "Try using a $\\lambda$-value of $2$ and see what happens to our polynomial regression fit and the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize theta \n",
    "pass\n",
    "\n",
    "# Optimize, reg = 2 \n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot \n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot (x: Change in water level, y: Water flowing out of the dam)\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Udated learning curve function with reg parameter \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the function to get the data \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result (x: Number of training examples, y: Loss)\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above plots show, we no longer fit the training set perfectly, however, our cross-validation error has significantly decreased to the point where it is almost the same as our training error. This is encouraging and an indicator of a model that can generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Regularize the model with high lambda\n",
    "\n",
    "What happens if we set $\\lambda$ up to something  high, $100$ for example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the theta \n",
    "pass \n",
    "\n",
    "# optimize, reg = 100 \n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model data for plot \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're back to the problem we had with our linear model; the model is underfit! When $\\lambda$ is too high, the values of $\\theta$ are pushed too close to zero and cannot fit the data well. As such, we do poorly on both the training and cross-validation data. Playing with the value of $\\lambda$ to find the best fit is the eternal struggle of machine learning, even in neural networks!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Discuss the effect of regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In written form, if you have finished in advance to the others. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
